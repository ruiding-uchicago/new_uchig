{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f29ee41-71df-4993-ba13-ff79e09946df",
   "metadata": {},
   "source": [
    "## PFAS Screening\n",
    "Okay, so here's the deal. We have 7 PFAS candidates as targets. We also have 765 probe material candidates. \n",
    "\n",
    "Edit: same idea we had for PFAS-v1. But now let's simplify the model a little bit and then for v4 we'll actually train it using only the original data and see if we get the same probe suggestions for PFOS detection.\n",
    "\n",
    "The idea now is:\n",
    "1. Go through all json files\n",
    "2. Enumerate all different types of medium and condtions\n",
    "3. Create two separate datasets: all_medium and all_conditions\n",
    "4. Combine elements of all_PFAS + all_probe + all_medium + all_conditions to get a list of LDLs\n",
    "5. Get the probe materials related to the lowest LDLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d34ee40-8480-42c5-8374-19dab511c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GraphConv, global_mean_pool\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import snntorch as snn\n",
    "\n",
    "# Category mappings for LDL as integers\n",
    "ldl_mapping = {\n",
    "    'Very Low Detection (High Sensitivity)': 0,\n",
    "    'Low Detection': 1,\n",
    "    'Moderate Detection': 2,\n",
    "    'High Detection (Lower Sensitivity)': 3,\n",
    "    'Very High Detection (Low Sensitivity)': 4\n",
    "}\n",
    "\n",
    "def extract_features(descriptors):\n",
    "    numerical_values = []\n",
    "    for key, value in descriptors.items():\n",
    "        if key == \"CID\":\n",
    "            continue\n",
    "        if isinstance(value, (int, float)):\n",
    "            numerical_values.append(float(value))\n",
    "        elif isinstance(value, str) and value.replace('.', '', 1).isdigit():\n",
    "            numerical_values.append(float(value))\n",
    "    return torch.tensor(numerical_values, dtype=torch.float)\n",
    "\n",
    "# Step 1: Find maximum feature length across JSON files\n",
    "def find_max_feature_length(json_files, folder_path):\n",
    "    max_feature_length = 0\n",
    "    for f in json_files:\n",
    "        data = json.load(open(os.path.join(folder_path, f), 'r'))\n",
    "        for section in ['detect_target', 'probe_material', 'test_medium_electrolyte']:\n",
    "            for item in data.get(section, []):\n",
    "                feature_length = len(extract_features(item['substance_descriptors']))\n",
    "                max_feature_length = max(max_feature_length, feature_length)\n",
    "    return max_feature_length\n",
    "\n",
    "# Updated helper function to extract features and include Magpie Descriptors for \"inorganic solid\"\n",
    "def extract_features_with_magpie(descriptors, substance_type):\n",
    "    numerical_values = []\n",
    "    for key, value in descriptors.items():\n",
    "        if key == \"CID\" or (substance_type == \"inorganic solid\" and \"MagpieData\" in key):\n",
    "            continue\n",
    "        if isinstance(value, (int, float)):\n",
    "            numerical_values.append(float(value))\n",
    "        elif isinstance(value, str) and value.replace('.', '', 1).isdigit():\n",
    "            numerical_values.append(float(value))\n",
    "    \n",
    "    # Add Magpie Descriptors for inorganic solids if they exist\n",
    "    if substance_type == \"inorganic solid\" and descriptors.get(\"Magpie Descriptors\") is not None:\n",
    "        magpie_descriptors = [\n",
    "            float(v) for k, v in descriptors[\"Magpie Descriptors\"].items() if isinstance(v, (int, float))\n",
    "        ]\n",
    "        numerical_values.extend(magpie_descriptors)\n",
    "    \n",
    "    return torch.tensor(numerical_values, dtype=torch.float)\n",
    "\n",
    "\n",
    "# Updated aggregation function to use the new extract_features_with_magpie function\n",
    "def aggregate_by_type(items, max_feature_length):\n",
    "    types = {'small molecule': [], 'inorganic solid': [], 'polymer': []}\n",
    "    for item in items:\n",
    "        substance_type = item.get('substance_type', '').lower()\n",
    "        if substance_type in types:\n",
    "            # Extract features, considering Magpie Descriptors if the type is \"inorganic solid\"\n",
    "            feature_tensor = F.pad(\n",
    "                extract_features_with_magpie(item['substance_descriptors'], substance_type),\n",
    "                (0, max_feature_length - len(extract_features_with_magpie(item['substance_descriptors'], substance_type)))\n",
    "            )\n",
    "            types[substance_type].append(feature_tensor)\n",
    "    \n",
    "    aggregated_features = []\n",
    "    for type_key in types:\n",
    "        if types[type_key]:\n",
    "            aggregated_features.append(torch.mean(torch.stack(types[type_key]), dim=0))\n",
    "        else:\n",
    "            aggregated_features.append(torch.zeros(max_feature_length))\n",
    "    \n",
    "    return torch.cat(aggregated_features)\n",
    "\n",
    "# Generate graph from JSON data (no changes needed here)\n",
    "def generate_graph_from_json(data, max_feature_length):\n",
    "    nodes = []\n",
    "    edges = []\n",
    "\n",
    "    # Aggregate target, probe, medium nodes with updated aggregation function\n",
    "    target_node = aggregate_by_type(data.get('detect_target', []), max_feature_length)\n",
    "    probe_node = aggregate_by_type(data.get('probe_material', []), max_feature_length)\n",
    "    medium_node = aggregate_by_type(data.get('test_medium_electrolyte', []), max_feature_length)\n",
    "\n",
    "    # Conditions node\n",
    "    conditions_features = [\n",
    "        data.get(\"test_operating_temperature_celsius\", 0.0),\n",
    "        data.get(\"min_pH_when_testing\", -1.0),\n",
    "        data.get(\"max_pH_when_testing\", 0.0)\n",
    "    ]\n",
    "    conditions_node = F.pad(torch.tensor(conditions_features, dtype=torch.float),\n",
    "                            (0, max_feature_length * 3 - len(conditions_features)))\n",
    "\n",
    "    nodes.extend([target_node, probe_node, medium_node, conditions_node])\n",
    "\n",
    "    edges = [\n",
    "        (0, 1),  # Target -> Probe\n",
    "        (0, 2),  # Target -> Medium\n",
    "        (1, 2),  # Probe -> Medium\n",
    "        (3, 2)   # Conditions -> Medium\n",
    "    ]\n",
    "\n",
    "    x = torch.stack(nodes)\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    ldl_label = ldl_mapping.get(data.get(\"LDL_category\", \"\"), 0)\n",
    "    return Data(x=x, edge_index=edge_index, y=torch.tensor(ldl_label, dtype=torch.long))\n",
    "\n",
    "# Load and process data\n",
    "folder_path = 'JSON_data/retrieved_substances_data_enhanced_json_files_2nd_attempt_11052024'\n",
    "json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "max_feature_length = find_max_feature_length(json_files, folder_path)\n",
    "\n",
    "# The rest of the data loading and processing code remains unchanged\n",
    "def extract_and_concatenate_vectors(substances):\n",
    "    vector_data = []\n",
    "    for substance in substances:\n",
    "        descriptors = substance.get('substance_descriptors', {})\n",
    "        for key in [\"Morgan_128\", \"maccs_fp\", \"morgan_fp_128\"]:\n",
    "            value = descriptors.get(key, [])\n",
    "            if isinstance(value, list):\n",
    "                vector_data.extend(value)\n",
    "    return np.array(vector_data)\n",
    "\n",
    "snn_data = []\n",
    "gnn_data = []\n",
    "labels = []\n",
    "\n",
    "# Find the maximum vector length across all files\n",
    "max_vector_length = 0\n",
    "all_vectors = []\n",
    "\n",
    "for f in json_files:\n",
    "    # Load JSON data\n",
    "    data = json.load(open(os.path.join(folder_path, f), 'r'))\n",
    "\n",
    "    # Generate graph and append to gnn_data\n",
    "    graph = generate_graph_from_json(data, max_feature_length)\n",
    "    if graph is not None:\n",
    "        gnn_data.append(graph)\n",
    "\n",
    "        # Generate the spiking vector by concatenating target, probe, and medium\n",
    "        spiking_vector = np.concatenate([\n",
    "            extract_and_concatenate_vectors(data.get('detect_target', [])),\n",
    "            extract_and_concatenate_vectors(data.get('probe_material', [])),\n",
    "            extract_and_concatenate_vectors(data.get('test_medium_electrolyte', []))\n",
    "        ])\n",
    "        all_vectors.append(spiking_vector)\n",
    "\n",
    "        # Update the maximum vector length if needed\n",
    "        max_vector_length = max(max_vector_length, len(spiking_vector))\n",
    "        # Append the label for classification\n",
    "        labels.append(graph.y.item())\n",
    "\n",
    "# Pad all_vectors to the maximum vector length\n",
    "padded_vectors = [np.pad(vec, (0, max_vector_length - len(vec)), 'constant') for vec in all_vectors]\n",
    "\n",
    "# Append each padded vector directly to snn_data\n",
    "snn_data.extend(padded_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58252c5b-3429-4fef-a9c0-cc09a2f2abcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rpf/anaconda3/envs/snn-gnn-env/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "snn_data = torch.FloatTensor(snn_data)\n",
    "labels = torch.LongTensor(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(snn_data, labels, test_size=0.2, random_state=42)\n",
    "train_data, test_data = train_test_split(gnn_data, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# Define SNN model with customizable beta\n",
    "class SNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, beta):\n",
    "        super(SNN, self).__init__()\n",
    "        self.beta = beta\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.beta * self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define GNN model with customizable number of layers\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes, num_layers):\n",
    "        super(GNN, self).__init__()\n",
    "        self.convs = nn.ModuleList([GraphConv(num_features if i == 0 else hidden_channels, hidden_channels) for i in range(num_layers)])\n",
    "        self.lin = nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69b2b64f-6b46-4619-ade1-221c6e6c4356",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, snn_input_size, snn_hidden_size, gnn_input_size, gnn_hidden_size, num_classes, snn_beta=0.9, gnn_layers=2):\n",
    "        super(MultiModalModel, self).__init__()\n",
    "        self.snn = SNN(snn_input_size, snn_hidden_size, num_classes, beta=snn_beta)\n",
    "        self.gnn = GNN(gnn_input_size, gnn_hidden_size, num_classes, num_layers=gnn_layers)\n",
    "        self.fusion_layer = nn.Linear(num_classes * 2, num_classes)\n",
    "\n",
    "    def forward(self, snn_input, gnn_data):\n",
    "        snn_output = self.snn(snn_input)\n",
    "        gnn_output = self.gnn(gnn_data)\n",
    "        combined_output = torch.cat((snn_output, gnn_output), dim=1)\n",
    "        final_output = self.fusion_layer(combined_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78a13c70-89b7-4eca-a845-654d1ee4006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multimodal(snn_loader, gnn_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (snn_batch, (gnn_data)) in zip(snn_loader, gnn_loader):\n",
    "        snn_input, snn_labels = snn_batch\n",
    "        snn_input, snn_labels = snn_input.to(device), snn_labels.to(device)\n",
    "        gnn_data = gnn_data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(snn_input, gnn_data)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, gnn_data.y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(snn_loader)\n",
    "\n",
    "def test_multimodal(snn_loader, gnn_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds, labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (snn_batch, gnn_data) in zip(snn_loader, gnn_loader):\n",
    "            snn_data, snn_labels = snn_batch\n",
    "            snn_data, snn_labels = snn_data.to(device), snn_labels.to(device)\n",
    "            gnn_data = gnn_data.to(device)\n",
    "            \n",
    "            # Forward pass through multi-modal model\n",
    "            output = model(snn_data, gnn_data)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output, gnn_data.y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Predictions and labels for metrics\n",
    "            pred = output.argmax(dim=1)\n",
    "            preds.extend(pred.cpu().numpy())  # Save predicted labels\n",
    "            labels.extend(gnn_data.y.cpu().numpy())  # Save true labels\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    avg_loss = total_loss / len(snn_loader)\n",
    "    \n",
    "    # Return evaluation metrics, predictions, and true labels\n",
    "    return avg_loss, accuracy, f1, precision, recall, preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e5b5d79-a9fc-40f5-b6c5-7d1526194851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and their configurations\n",
    "def save_model(model, file_path, description):\n",
    "    \"\"\"\n",
    "    Save the trained model and print its characteristics.\n",
    "    \"\"\"\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), file_path)\n",
    "    \n",
    "    # Print model characteristics\n",
    "    print(f\"Model saved to {file_path}\")\n",
    "    print(\"Model Description:\")\n",
    "    print(description)\n",
    "    print(\"Number of Parameters:\", sum(p.numel() for p in model.parameters()))\n",
    "    print(\"Trainable Parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c0b7be7-c68c-4311-96d1-274a292bd004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming gnn_data[0].x.shape[1] gives the correct input feature size for GNN\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gnn_input_size = gnn_data[0].x.shape[1]\n",
    "\n",
    "model = MultiModalModel(\n",
    "    snn_input_size=snn_data.shape[1],  # 1358 based on snn_data.shape\n",
    "    snn_hidden_size=64,\n",
    "    gnn_input_size=gnn_input_size,     # Updated to match the actual feature size of gnn_data.x\n",
    "    gnn_hidden_size=64,\n",
    "    num_classes=5,\n",
    "    snn_beta=0.85,\n",
    "    gnn_layers=7\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c39891c4-dce6-4682-8a40-740474f6ada0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/200, Training Loss: 1.5019\n",
      "Epoch 5/200, Training Loss: 1.0610\n",
      "Epoch 10/200, Training Loss: 0.8368\n",
      "Epoch 15/200, Training Loss: 0.6164\n",
      "Epoch 20/200, Training Loss: 0.4844\n",
      "Epoch 25/200, Training Loss: 0.3906\n",
      "Epoch 30/200, Training Loss: 0.3590\n",
      "Epoch 35/200, Training Loss: 0.3496\n",
      "Epoch 40/200, Training Loss: 0.3021\n",
      "Epoch 45/200, Training Loss: 0.3060\n",
      "Epoch 50/200, Training Loss: 0.2797\n",
      "Epoch 55/200, Training Loss: 0.2847\n",
      "Epoch 60/200, Training Loss: 0.2630\n",
      "Epoch 65/200, Training Loss: 0.2592\n",
      "Epoch 70/200, Training Loss: 0.2684\n",
      "Epoch 75/200, Training Loss: 0.2466\n",
      "Epoch 80/200, Training Loss: 0.2763\n",
      "Epoch 85/200, Training Loss: 0.2526\n",
      "Epoch 90/200, Training Loss: 0.2577\n",
      "Epoch 95/200, Training Loss: 0.2583\n",
      "Epoch 100/200, Training Loss: 0.2406\n",
      "Epoch 105/200, Training Loss: 0.2503\n",
      "Epoch 110/200, Training Loss: 0.2330\n",
      "Epoch 115/200, Training Loss: 0.2554\n",
      "Epoch 120/200, Training Loss: 0.2321\n",
      "Epoch 125/200, Training Loss: 0.2232\n",
      "Epoch 130/200, Training Loss: 0.2450\n",
      "Epoch 135/200, Training Loss: 0.2601\n",
      "Epoch 140/200, Training Loss: 0.2229\n",
      "Epoch 145/200, Training Loss: 0.2604\n",
      "Epoch 150/200, Training Loss: 0.2214\n",
      "Epoch 155/200, Training Loss: 0.2374\n",
      "Epoch 160/200, Training Loss: 0.2183\n",
      "Epoch 165/200, Training Loss: 0.2245\n",
      "Epoch 170/200, Training Loss: 0.2095\n",
      "Epoch 175/200, Training Loss: 0.2143\n",
      "Epoch 180/200, Training Loss: 0.2206\n",
      "Epoch 185/200, Training Loss: 0.2161\n",
      "Epoch 190/200, Training Loss: 0.2307\n",
      "Epoch 195/200, Training Loss: 0.2173\n",
      "Test Loss: 0.2378\n",
      "Accuracy: 0.8863\n",
      "F1 Score: 0.8868\n",
      "Precision: 0.8919\n",
      "Recall: 0.8863\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader for SNN data with matching batch size\n",
    "snn_train_loader = DataLoader(list(zip(X_train, y_train)), batch_size=64, shuffle=True)\n",
    "snn_test_loader = DataLoader(list(zip(X_test, y_test)), batch_size=64)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_multimodal(snn_train_loader, train_loader)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Final evaluation using test data loaders\n",
    "test_loss, accuracy, f1, precision, recall, preds, labels = test_multimodal(snn_test_loader, test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69742591-cbee-4d21-b14c-2ed5af57cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder path\n",
    "folder_path = 'JSON_data/retrieved_substances_data_enhanced_json_files_2nd_attempt_11052024'\n",
    "\n",
    "# List to hold all files ending with '_original.json'\n",
    "json_files_org = [file for file in os.listdir(folder_path) if file.endswith('_original.json')]\n",
    "max_feature_length_org = find_max_feature_length(json_files_org, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84e97844-2220-4fba-9aed-d2bdadcb6013",
   "metadata": {},
   "outputs": [],
   "source": [
    "snn_data = []\n",
    "gnn_data = []\n",
    "labels = []\n",
    "\n",
    "# Find the maximum vector length across all files\n",
    "max_vector_length = 0\n",
    "all_vectors = []\n",
    "\n",
    "for f in json_files_org:\n",
    "    file_path = os.path.join(folder_path, f)  # Construct the full path to the file\n",
    "    \n",
    "    # Load JSON data\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # Generate graph and append to gnn_data\n",
    "    graph = generate_graph_from_json(data, max_feature_length_org)\n",
    "    if graph is not None:\n",
    "        gnn_data.append(graph)\n",
    "\n",
    "        # Generate the spiking vector by concatenating target, probe, and medium\n",
    "        spiking_vector = np.concatenate([\n",
    "            extract_and_concatenate_vectors(data.get('detect_target', [])),\n",
    "            extract_and_concatenate_vectors(data.get('probe_material', [])),\n",
    "            extract_and_concatenate_vectors(data.get('test_medium_electrolyte', []))\n",
    "        ])\n",
    "        all_vectors.append(spiking_vector)\n",
    "\n",
    "        # Update the maximum vector length if needed\n",
    "        max_vector_length = max(max_vector_length, len(spiking_vector))\n",
    "\n",
    "        # Append the label for classification\n",
    "        labels.append(graph.y.item())\n",
    "\n",
    "# Pad all_vectors to the maximum vector length\n",
    "padded_vectors = [np.pad(vec, (0, max_vector_length - len(vec)), 'constant') for vec in all_vectors]\n",
    "\n",
    "# Append each padded vector directly to snn_data\n",
    "snn_data.extend(padded_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74d3dc84-e67f-45ab-b8e2-2e6d5d2e22dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rpf/anaconda3/envs/snn-gnn-env/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "snn_data_org = torch.FloatTensor(snn_data)\n",
    "labels_org = torch.LongTensor(labels)\n",
    "\n",
    "_, X_test, _, y_test = train_test_split(snn_data_org, labels_org, test_size=0.999, random_state=42)\n",
    "_, test_data = train_test_split(gnn_data, test_size=0.999, random_state=42)\n",
    "#train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# Create DataLoader for SNN data with matching batch size\n",
    "#snn_train_loader = DataLoader(list(zip(X_train, y_train)), batch_size=64, shuffle=True)\n",
    "snn_test_loader = DataLoader(list(zip(X_test, y_test)), batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0e20844-b69f-48a9-9190-190ecce4b06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1912\n",
      "Accuracy: 0.9068\n",
      "F1 Score: 0.9072\n",
      "Precision: 0.9096\n",
      "Recall: 0.9068\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation using test data loaders\n",
    "test_loss, accuracy, f1, precision, recall, preds, labels = test_multimodal(snn_test_loader, test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8ff2674-51a3-4eeb-95e1-9f88f24280b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1192 JSON files with fixed detect_target.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load the fixed detect_target from PFAS1.json\n",
    "with open('PFAS_Screening/PFAS_detect_target_6.json', 'r') as pfas_file:\n",
    "    pfas_data = json.load(pfas_file)\n",
    "fixed_detect_target = pfas_data.get('detect_target', [])\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = 'JSON_data/retrieved_substances_data_enhanced_json_files_2nd_attempt_11052024'\n",
    "\n",
    "# List to hold all files ending with '_original.json'\n",
    "json_files_org = [file for file in os.listdir(folder_path) if file.endswith('_original.json')]\n",
    "\n",
    "# Initialize data storage\n",
    "snn_data = []\n",
    "gnn_data = []\n",
    "labels = []\n",
    "\n",
    "# Function to extract and concatenate vector data\n",
    "def extract_and_concatenate_vectors(substances):\n",
    "    vector_data = []\n",
    "    for substance in substances:\n",
    "        descriptors = substance.get('substance_descriptors', {})\n",
    "        for key in [\"Morgan_128\", \"maccs_fp\", \"morgan_fp_128\"]:\n",
    "            value = descriptors.get(key, [])\n",
    "            if isinstance(value, list):\n",
    "                vector_data.extend(value)\n",
    "    return np.array(vector_data)\n",
    "\n",
    "# Iterate through JSON files\n",
    "max_vector_length = 0\n",
    "all_vectors = []\n",
    "\n",
    "for f in json_files_org:\n",
    "    file_path = os.path.join(folder_path, f)  # Construct the full path to the file\n",
    "    \n",
    "    # Load JSON data\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # Inject the fixed detect_target into the current data dictionary\n",
    "    data['detect_target'] = fixed_detect_target\n",
    "\n",
    "    # Aggregate target, probe, and medium nodes\n",
    "    probe_data = data.get('probe_material', [])\n",
    "    medium_data = data.get('test_medium_electrolyte', [])\n",
    "\n",
    "    # Concatenate vectors from detect_target, probe, and medium\n",
    "    spiking_vector = np.concatenate([\n",
    "        extract_and_concatenate_vectors(data.get('detect_target', [])),\n",
    "        extract_and_concatenate_vectors(probe_data),\n",
    "        extract_and_concatenate_vectors(medium_data)\n",
    "    ])\n",
    "    all_vectors.append(spiking_vector)\n",
    "\n",
    "    # Update the maximum vector length if needed\n",
    "    max_vector_length = max(max_vector_length, len(spiking_vector))\n",
    "\n",
    "    # Generate graph data\n",
    "    graph = generate_graph_from_json(data, max_feature_length_org)\n",
    "    if graph is not None:\n",
    "        gnn_data.append(graph)\n",
    "        labels.append(graph.y.item())\n",
    "\n",
    "# Pad all_vectors to the maximum vector length\n",
    "padded_vectors = [np.pad(vec, (0, max_vector_length - len(vec)), 'constant') for vec in all_vectors]\n",
    "\n",
    "# Append each padded vector directly to snn_data\n",
    "snn_data.extend(padded_vectors)\n",
    "\n",
    "print(f\"Processed {len(json_files_org)} JSON files with fixed detect_target.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f7b75b9-ba3b-4dd0-871c-018c8f2e58ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rpf/anaconda3/envs/snn-gnn-env/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "snn_data_org = torch.FloatTensor(snn_data)\n",
    "labels_org = torch.LongTensor(labels)\n",
    "\n",
    "_, X_test, _, y_test = train_test_split(snn_data_org, labels_org, test_size=0.999, random_state=42)\n",
    "_, test_data = train_test_split(gnn_data, test_size=0.999, random_state=42)\n",
    "#train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# Create DataLoader for SNN data with matching batch size\n",
    "#snn_train_loader = DataLoader(list(zip(X_train, y_train)), batch_size=64, shuffle=True)\n",
    "snn_test_loader = DataLoader(list(zip(X_test, y_test)), batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd490bb3-457e-46af-b60c-7a4960e91f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation using test data loaders\n",
    "_, _, _, _, _, preds, _ = test_multimodal(snn_test_loader, test_loader)\n",
    "#print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5622924-0eb0-458d-af0d-e64e8686dc09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions == 0: 195\n",
      "Size of filtered dataset: 89\n",
      "Recurrent Probe Material Names (Filtered) (Decreasing Order):\n",
      "zinc oxide: 10 occurrences\n",
      "graphene: 8 occurrences\n",
      "indium oxide: 7 occurrences\n",
      "aluminum oxide: 5 occurrences\n",
      "tin dioxide: 5 occurrences\n",
      "carbon nanotube: 5 occurrences\n",
      "gallium oxide: 4 occurrences\n",
      "gold: 3 occurrences\n",
      "phenol: 3 occurrences\n",
      "ethylene oxide: 3 occurrences\n",
      "benzoic acid: 3 occurrences\n",
      "fluorine: 2 occurrences\n",
      "tin(iv) oxide: 2 occurrences\n",
      "hafnium oxide: 2 occurrences\n",
      "diamond: 2 occurrences\n",
      "silicon nitride: 2 occurrences\n",
      "silicon dioxide: 2 occurrences\n",
      "polypyrrole: 2 occurrences\n",
      "valinomycin: 2 occurrences\n",
      "polydithiopropane sulfonic acid: 1 occurrences\n",
      "cerium oxide: 1 occurrences\n",
      "yttrium oxide: 1 occurrences\n",
      "poly(3-amino-benzylamine-co-aniline): 1 occurrences\n",
      "hafnium nitride: 1 occurrences\n",
      "ethyl undecylenate: 1 occurrences\n",
      "bismuth(iii) vanadate: 1 occurrences\n",
      "platinum: 1 occurrences\n",
      "triethylamine: 1 occurrences\n",
      "bismuth oxide: 1 occurrences\n",
      "l-cysteine: 1 occurrences\n",
      "tetraxetan: 1 occurrences\n",
      "4-carboxyphenylboronic acid: 1 occurrences\n",
      "tetraethyl orthosilicate: 1 occurrences\n",
      "3-aminopropyltriethoxysilane: 1 occurrences\n",
      "tetracycline: 1 occurrences\n",
      "titanium nitride: 1 occurrences\n",
      "polyvinyl chloride: 1 occurrences\n",
      "silicon: 1 occurrences\n",
      "cadmium ionophore i: 1 occurrences\n",
      "cysteamine: 1 occurrences\n",
      "2,7,12,17,22-penta-tert-butyl-4,9,14,19,24-pentacyano-pentabenzo[ab,fg,kl,pq,uv]-[25]annulene: 1 occurrences\n",
      "nickel silicide: 1 occurrences\n",
      "4-formylphenylboronic acid: 1 occurrences\n",
      "phenylboronic acid: 1 occurrences\n",
      "nickel-histidine: 1 occurrences\n",
      "zirconium oxide: 1 occurrences\n",
      "hafnium dioxide: 1 occurrences\n",
      "adenosine triphosphate: 1 occurrences\n",
      "cu: 1 occurrences\n",
      "4,4'-[1,4-phenylenebis(ethyne-2,1-diyl)]dibenzoic acid: 1 occurrences\n",
      "ethanol: 1 occurrences\n",
      "c8-btbt: 1 occurrences\n",
      "molybdenum disulfide: 1 occurrences\n",
      "dithiothreitol: 1 occurrences\n",
      "au: 1 occurrences\n",
      "molybdenum diselenide: 1 occurrences\n",
      "dysprosium(iii) titanium dioxide: 1 occurrences\n",
      "boron: 1 occurrences\n",
      "dextran: 1 occurrences\n",
      "silver: 1 occurrences\n",
      "polydimethylsiloxane: 1 occurrences\n",
      "vanadium pentoxide: 1 occurrences\n",
      "tungsten trioxide: 1 occurrences\n",
      "18-crown-6: 1 occurrences\n",
      "thioformaldehyde: 1 occurrences\n",
      "mono-(6-amino-6-deoxy)-beta-cyclodextrin: 1 occurrences\n",
      "(3-aminopropyl)triethoxysilane: 1 occurrences\n",
      "sodium ionophore x: 1 occurrences\n",
      "ionophore: 1 occurrences\n",
      "poly(vinyl chloride): 1 occurrences\n",
      "hydrocortisone 3-(o-carboxymethyl)oxime: 1 occurrences\n",
      "poly(3,4-ethylenedioxythiophene): 1 occurrences\n",
      "palladium oxide: 1 occurrences\n",
      "poly(3-hexylthiophene-2,5-diyl): 1 occurrences\n",
      "titanium carbide: 1 occurrences\n",
      "Recurrent Conditions Vectors (Filtered) (Decreasing Order):\n",
      "(25.0, 7.0, 7.0): 25 occurrences\n",
      "(25.0, 7.4, 7.4): 21 occurrences\n",
      "(25.0, 2.0, 12.0): 9 occurrences\n",
      "(25.0, 4.0, 10.0): 4 occurrences\n",
      "(25.0, 7.2, 7.2): 3 occurrences\n",
      "(25.0, 5.0, 5.0): 2 occurrences\n",
      "(25.0, 2.0, 10.0): 2 occurrences\n",
      "(25.0, 3.0, 11.0): 2 occurrences\n",
      "(25.0, 3.3, 11.4): 1 occurrences\n",
      "(25.0, 6.0, 6.0): 1 occurrences\n",
      "(25.0, 2.0, 8.0): 1 occurrences\n",
      "(25.0, 6.0, 12.0): 1 occurrences\n",
      "(25.0, 7.7, 7.7): 1 occurrences\n",
      "(25.0, 3.9, 9.8): 1 occurrences\n",
      "(25.0, 3.07, 9.870000000000001): 1 occurrences\n",
      "(25.0, 4.0, 9.0): 1 occurrences\n",
      "(25.0, 1.0, 13.0): 1 occurrences\n",
      "(25.0, -0.3996737210000001, 5.300162274): 1 occurrences\n",
      "(25.0, 4.7, 7.5): 1 occurrences\n",
      "(25.0, 5.7, 12.0): 1 occurrences\n",
      "(25.0, 5.0, 9.0): 1 occurrences\n",
      "(25.0, 8.5, 8.5): 1 occurrences\n",
      "(25.0, 7.33, 7.33): 1 occurrences\n",
      "(25.0, 2.8, 4.9): 1 occurrences\n",
      "(30.0, 2.0, 10.0): 1 occurrences\n",
      "(37.0, 7.4, 7.4): 1 occurrences\n",
      "(25.0, 3.0, 10.0): 1 occurrences\n",
      "(25.0, 8.19, 8.19): 1 occurrences\n",
      "(27.0, 2.0, 12.0): 1 occurrences\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Step 1: Initialize variables\n",
    "folder_path = 'JSON_data/retrieved_substances_data_enhanced_json_files_2nd_attempt_11052024'\n",
    "\n",
    "# Step 2: Collect details of instances where preds == 0\n",
    "zero_pred_details = []\n",
    "\n",
    "for idx, pred in enumerate(preds):\n",
    "    if pred == 0:\n",
    "        file_name = json_files_org[idx]\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "\n",
    "        probe_material = json_data.get('probe_material', [])\n",
    "        probe_material_names = [\n",
    "            item.get('substance_name', 'Unknown') for item in probe_material\n",
    "        ]\n",
    "\n",
    "        test_medium = json_data.get('test_medium_electrolyte', [])\n",
    "        test_medium_names = [\n",
    "            item.get('substance_name', 'Unknown') for item in test_medium\n",
    "        ]\n",
    "\n",
    "        conditions_vector = [\n",
    "            json_data.get(\"test_operating_temperature_celsius\", 0.0),\n",
    "            json_data.get(\"min_pH_when_testing\", -1.0),\n",
    "            json_data.get(\"max_pH_when_testing\", 0.0)\n",
    "        ]\n",
    "\n",
    "        zero_pred_details.append({\n",
    "            \"file_name\": file_name,\n",
    "            \"probe_material_names\": probe_material_names,\n",
    "            \"test_medium_names\": test_medium_names,\n",
    "            \"conditions_vector\": conditions_vector\n",
    "        })\n",
    "\n",
    "# Step 3: Print total instances where preds == 0\n",
    "print(f\"Number of predictions == 0: {len(zero_pred_details)}\")\n",
    "\n",
    "# Step 4: Filter out cases where conditions vector ends with [-1.0, -1.0]\n",
    "filtered_zero_pred_details = [\n",
    "    detail for detail in zero_pred_details if detail['conditions_vector'][-2:] != [-1.0, -1.0]\n",
    "]\n",
    "\n",
    "# Step 5: Print size of filtered dataset\n",
    "print(f\"Size of filtered dataset: {len(filtered_zero_pred_details)}\")\n",
    "\n",
    "# Step 6: Count and sort occurrences of probe materials\n",
    "filtered_probe_material_names = [\n",
    "    name for detail in filtered_zero_pred_details for name in detail['probe_material_names']\n",
    "]\n",
    "filtered_probe_material_counts = Counter(filtered_probe_material_names)\n",
    "sorted_filtered_probe_material_counts = sorted(filtered_probe_material_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Step 7: Print occurrences of probe materials\n",
    "print(\"Recurrent Probe Material Names (Filtered) (Decreasing Order):\")\n",
    "for name, count in sorted_filtered_probe_material_counts:\n",
    "    print(f\"{name}: {count} occurrences\")\n",
    "\n",
    "\n",
    "# Step 8: Count and sort occurrences of conditions\n",
    "filtered_conditions = [\n",
    "    tuple(detail['conditions_vector']) for detail in filtered_zero_pred_details\n",
    "]\n",
    "filtered_conditions_counts = Counter(filtered_conditions)\n",
    "sorted_filtered_conditions_counts = sorted(filtered_conditions_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print occurrences of conditions\n",
    "print(\"Recurrent Conditions Vectors (Filtered) (Decreasing Order):\")\n",
    "for conditions, count in sorted_filtered_conditions_counts:\n",
    "    print(f\"{conditions}: {count} occurrences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806da10a-8517-4ce0-b16f-cc615b546f96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNN-GNN Env",
   "language": "python",
   "name": "snn-gnn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
